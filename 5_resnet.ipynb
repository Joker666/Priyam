{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f06a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dde76f",
   "metadata": {},
   "source": [
    "## Increase number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8d3157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 32, 32])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(4, 16, 32, 32)\n",
    "conv = nn.Conv2d(in_channels=16, out_channels=256, kernel_size=1)\n",
    "conv(rand).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72050a1a",
   "metadata": {},
   "source": [
    "## Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614ef683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, downsample=None, middle_conv_stride=1, residual=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        ### Set Convolutional Layers ###\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=middle_conv_stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        ### Output to planes * 4 as our expansion ###\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        ### This Will Exist if a Downsample Is Needed ###\n",
    "        self.downsample = downsample\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.residual:\n",
    "            if self.downsample is not None:  # If our identity function has less channels or larger size we remap it\n",
    "                identity = self.downsample(identity)\n",
    "\n",
    "            x = x + identity\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc44cb",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2708f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, layer_counts, num_channels=3, num_classes=2, residual=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.residual = residual\n",
    "\n",
    "        self.in_planes = 64  # Starting number of planes to map to from input channels\n",
    "\n",
    "        ### INITIAL SET OF CONVOLUTIONS ###\n",
    "        self.conv1 = nn.Conv2d(num_channels, self.in_planes, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        ### DEFINE LAYERS ###\n",
    "        self.layer1 = self._make_layers(layer_counts[0], 64, stride=1)\n",
    "        self.layer2 = self._make_layers(layer_counts[1], 128, stride=2)\n",
    "        self.layer3 = self._make_layers(layer_counts[2], 256, stride=2)\n",
    "        self.layer4 = self._make_layers(layer_counts[3], 512, stride=2)\n",
    "\n",
    "        ### AVERAGE POOLING AND MAP TO CLASSIFIER ###\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def _make_layers(self, num_residual_blocks, planes, stride=1):\n",
    "        downsample = None\n",
    "\n",
    "        # If we have a stride of 2, or the number of planes dont match. This condition will ALWAYS BE MET only\n",
    "        # on the first block of every set of blocks\n",
    "        if self.residual and (stride != 1 or self.in_planes != planes * 4):\n",
    "            ### Map to the number of wanted planes with a stride of 2 to map identity to X\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes * 4, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes * 4),\n",
    "            )\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        ### Append this First Block with the Downsample Layer ###\n",
    "        layers.append(ResidualBlock(self.in_planes, planes, downsample, stride, residual=self.residual))\n",
    "\n",
    "        ### Set our InPlanes to be expanded by 4 ###\n",
    "        self.in_planes = planes * 4\n",
    "\n",
    "        ### The remaining layers shouldnt have any issues so we can just append all of teh blocks on ###\n",
    "        for _ in range(1, num_residual_blocks):\n",
    "            layers.append(ResidualBlock(self.in_planes, planes, residual=self.residual))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x).squeeze()\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def ResNet50(num_channels=3, num_classes=2, residual=True):\n",
    "    return ResNet([3, 4, 6, 3], num_channels, num_classes, residual)\n",
    "\n",
    "\n",
    "def ResNet101(num_channels=3, num_classes=2, residual=True):\n",
    "    return ResNet([3, 4, 23, 3], num_channels, num_classes, residual)\n",
    "\n",
    "\n",
    "def ResNet152(num_channels=3, num_classes=2, residual=True):\n",
    "    return ResNet([3, 8, 36, 3], num_channels, num_classes, residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e84458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50(residual=True)\n",
    "rand = torch.randn(4, 3, 224, 224)\n",
    "model(rand).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf9596",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a327747",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps:0\"\n",
    "path_to_data = \"./catsanddogs/PetImages/\"\n",
    "\n",
    "normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalizer,\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ImageFolder(path_to_data, transform=train_transform)\n",
    "\n",
    "train_samples, test_samples = int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths=[train_samples, test_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c55265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffea945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, loss_fn, train_loader, val_loader):\n",
    "    log_training = {\n",
    "        \"epoch\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"training_acc\": [],\n",
    "        \"validation_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        train_losses, train_accuracies = [], []\n",
    "        val_losses, val_accuracies = [], []\n",
    "\n",
    "        model.train()\n",
    "        for image, label in tqdm(train_loader, desc=\"Training\"):\n",
    "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = loss_fn(output, label)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Compute accuracy\n",
    "            predictions = torch.argmax(output, axis=-1)\n",
    "            accuracy = (predictions == label).float().mean()\n",
    "            train_accuracies.append(accuracy.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        for image, label in tqdm(val_loader, desc=\"Validation\"):\n",
    "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "                loss = loss_fn(output, label)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                # Compute accuracy\n",
    "                predictions = torch.argmax(output, axis=-1)\n",
    "                accuracy = (predictions == label).float().mean()\n",
    "                val_accuracies.append(accuracy.item())\n",
    "\n",
    "        training_loss_mean, training_acc_mean = np.mean(train_losses), np.mean(train_accuracies)\n",
    "        valid_loss_mean, valid_acc_mean = np.mean(val_losses), np.mean(val_accuracies)\n",
    "\n",
    "        log_training[\"epoch\"].append(epoch)\n",
    "        log_training[\"training_loss\"].append(training_loss_mean)\n",
    "        log_training[\"training_acc\"].append(training_acc_mean)\n",
    "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
    "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
    "\n",
    "        print(\"Training Loss:\", training_loss_mean)\n",
    "        print(\"Training Acc:\", training_acc_mean)\n",
    "        print(\"Validation Loss:\", valid_loss_mean)\n",
    "        print(\"Validation Acc:\", valid_acc_mean)\n",
    "\n",
    "    return log_training, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9975325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training With Residuals\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1a9907f1fe472b95063ec6fd890d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining With Residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m resnet_w_res, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, optimizer, loss_fn, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m==\u001b[39m label)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     27\u001b[0m train_accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/3.12.9/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/3.12.9/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/3.12.9/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training With Residuals\")\n",
    "resnet_w_res, model = train(model, EPOCHS, optimizer, loss_fn, train_loader, val_loader)\n",
    "\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acff40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2d44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a131fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ce0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7ab9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622117d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2e655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
