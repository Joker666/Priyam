{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 126, 126])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(4, 3, 128, 128)\n",
    "\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "conv(rand).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Convolution Shape: torch.Size([64, 62, 62])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(3, 128, 128)\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, stride=2, padding=1)\n",
    "conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "output = conv2(conv1(rand))\n",
    "\n",
    "print(\"Stacked Convolution Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 122, 122])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(MyConv2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.linear = nn.Linear(in_channels * kernel_size * kernel_size, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        assert channels == self.in_channels\n",
    "\n",
    "        # Calculate output dimensions once\n",
    "        out_height = (height - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        out_width = (width - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        # Use unfold to extract patches\n",
    "        patched = nn.functional.unfold(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        _, kernel_param_size, num_patches = patched.shape\n",
    "\n",
    "        # Reshape for linear operation more efficiently\n",
    "        patched = patched.transpose(1, 2).reshape(-1, self.in_channels * self.kernel_size * self.kernel_size)\n",
    "\n",
    "        # Apply linear transformation\n",
    "        convolved = self.linear(patched)\n",
    "\n",
    "        # Reshape back to proper output format\n",
    "        convolved = convolved.reshape(batch_size, num_patches, -1)\n",
    "        convolved = convolved.transpose(1, 2).reshape(batch_size, self.out_channels, out_height, out_width)\n",
    "        return convolved\n",
    "\n",
    "\n",
    "rand = torch.randn(4, 3, 128, 128)\n",
    "conv = MyConv2d(3, 64, 7)\n",
    "conv(rand).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 42, 42]) torch.Size([4, 3, 63, 63])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.randn(4, 3, 128, 128)\n",
    "\n",
    "avg_pool = nn.AvgPool2d(kernel_size=3)\n",
    "avg_pool_2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "out = avg_pool(rand)\n",
    "out_2 = avg_pool_2(rand)\n",
    "\n",
    "print(out.shape, out_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "adative_pooling = torch.nn.AdaptiveAvgPool2d((64, 64))\n",
    "print(adative_pooling(rand).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, dropout=0.5):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, self.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps:0\"\n",
    "path_to_data = \"./catsanddogs/PetImages/\"\n",
    "\n",
    "normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalizer,\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ImageFolder(path_to_data, transform=train_transform)\n",
    "\n",
    "train_samples, test_samples = int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths=[train_samples, test_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, loss_fn, train_loader, val_loader):\n",
    "    log_training = {\n",
    "        \"epoch\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"training_acc\": [],\n",
    "        \"validation_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        train_losses, train_accuracies = [], []\n",
    "        val_losses, val_accuracies = [], []\n",
    "\n",
    "        model.train()\n",
    "        for image, label in tqdm(train_loader, desc=\"Training\"):\n",
    "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = loss_fn(output, label)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Compute accuracy\n",
    "            predictions = torch.argmax(output, axis=-1)\n",
    "            accuracy = (predictions == label).float().mean()\n",
    "            train_accuracies.append(accuracy.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        for image, label in tqdm(val_loader, desc=\"Validation\"):\n",
    "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "                loss = loss_fn(output, label)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                # Compute accuracy\n",
    "                predictions = torch.argmax(output, axis=-1)\n",
    "                accuracy = (predictions == label).float().mean()\n",
    "                val_accuracies.append(accuracy.item())\n",
    "\n",
    "        training_loss_mean, training_acc_mean = np.mean(train_losses), np.mean(train_accuracies)\n",
    "        valid_loss_mean, valid_acc_mean = np.mean(val_losses), np.mean(val_accuracies)\n",
    "\n",
    "        log_training[\"epoch\"].append(epoch)\n",
    "        log_training[\"training_loss\"].append(training_loss_mean)\n",
    "        log_training[\"training_acc\"].append(training_acc_mean)\n",
    "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
    "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
    "\n",
    "        print(\"Training Loss:\", training_loss_mean)\n",
    "        print(\"Training Acc:\", training_acc_mean)\n",
    "        print(\"Validation Loss:\", valid_loss_mean)\n",
    "        print(\"Validation Acc:\", valid_acc_mean)\n",
    "\n",
    "    return log_training, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35d176ab101426d8e6e781c4a04638c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f75be88c5f4801b2d4e132ed5ba738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafi/.local/share/mise/installs/python/3.12.9/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6230423291298476\n",
      "Training Acc: 0.6901651641184633\n",
      "Validation Loss: 0.49977565109729766\n",
      "Validation Acc: 0.7576056987047195\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbadd1412554bb9b577d5774c5bcc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7f5fa9e35240d9a64dccdbe84dd77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4218318970365958\n",
      "Training Acc: 0.8063652308827097\n",
      "Validation Loss: 0.35902311056852343\n",
      "Validation Acc: 0.8410845577716828\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f25ac2b4d74f9bb77034efdf14c0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699edf96a5b9424198acca49cfcbede5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3441081251949072\n",
      "Training Acc: 0.84731200709939\n",
      "Validation Loss: 0.3253169983625412\n",
      "Validation Acc: 0.8520220577716827\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca1e6fc5aa7487caa65729457dfb1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f28fd229d94a8cad22e71aa1393019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.30165519438345323\n",
      "Training Acc: 0.8697457675906745\n",
      "Validation Loss: 0.2680665396153927\n",
      "Validation Acc: 0.8786305159330368\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5592018713f8494e87573c39c2d58cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ff2dae45314adaa3a18901126b173c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2594657489521937\n",
      "Training Acc: 0.889051447876475\n",
      "Validation Loss: 0.2322740450501442\n",
      "Validation Acc: 0.8994944840669632\n"
     ]
    }
   ],
   "source": [
    "random_init_log, model = train(model, EPOCHS, optimizer, loss_fn, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
