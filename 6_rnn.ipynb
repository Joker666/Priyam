{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN + LSTM\n",
    "\n",
    "[Code](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20NLP/Recurrent%20Neural%20Networks/IMDB%20Classification/Sequence%20Classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords') #Download the NLTK Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:04<00:00, 5443.88it/s]\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"./aclImdb/train\"\n",
    "\n",
    "path_to_pos_folder = os.path.join(path_to_data, \"pos\")\n",
    "path_to_neg_folder = os.path.join(path_to_data, \"neg\")\n",
    "\n",
    "path_to_pos_txt = [os.path.join(path_to_pos_folder, file) for file in os.listdir(path_to_pos_folder)]\n",
    "path_to_neg_txt = [os.path.join(path_to_neg_folder, file) for file in os.listdir(path_to_neg_folder)]\n",
    "\n",
    "training_files = path_to_pos_txt + path_to_neg_txt\n",
    "\n",
    "all_text = []\n",
    "len_words = []\n",
    "\n",
    "for file in tqdm(training_files):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()[0].lower()\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove All Punctuation\n",
    "        text = text.split(\" \")  # Split by Space\n",
    "        text = [word for word in text if word not in stopwords]  # Remove Stopwords\n",
    "\n",
    "        len_words.append(len(text))\n",
    "        all_text += text\n",
    "\n",
    "unique_counts = dict(Counter(all_text))\n",
    "words = sorted([key for key, value in unique_counts.items() if value > 500])\n",
    "\n",
    "words.append(\"<unk>\")\n",
    "words.append(\"<pad>\")\n",
    "\n",
    "word2index = {word: i for i, word in enumerate(words)}\n",
    "index2word = {i: word for i, word in enumerate(words)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[831, 794, 424,  ..., 991, 991, 991],\n",
      "        [990, 990, 990,  ..., 520, 990, 990],\n",
      "        [990, 740, 762,  ..., 991, 991, 991],\n",
      "        ...,\n",
      "        [990, 990, 517,  ..., 991, 991, 991],\n",
      "        [679,   1,  79,  ..., 991, 991, 991],\n",
      "        [884, 990, 295,  ..., 991, 991, 991]]), tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, training_files, word2index, max_seq_length=200):\n",
    "        self.training_files = training_files\n",
    "        self.tokenizer = word2index\n",
    "        self.max_len = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_text = self.training_files[idx]\n",
    "        with open(path_to_text, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "            text = text.split(\" \")\n",
    "            text = [word for word in text if word not in stopwords]\n",
    "            tokenized = [self.tokenizer.get(word, self.tokenizer[\"<unk>\"]) for word in text]\n",
    "            sample = torch.tensor(tokenized)\n",
    "\n",
    "            if len(sample) > self.max_len:\n",
    "                diff = len(sample) - self.max_len\n",
    "                start_idx = np.random.randint(0, diff)\n",
    "                sample = sample[start_idx : start_idx + self.max_len]\n",
    "\n",
    "            if \"neg\" in path_to_text:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            return sample, label\n",
    "\n",
    "\n",
    "dataset = IMDBDataset(training_files, word2index)\n",
    "\n",
    "\n",
    "def data_collator(batch):\n",
    "    texts, labels = [], []\n",
    "    for text, label in batch:\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    label = torch.tensor(labels)\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=word2index[\"<pad>\"])\n",
    "    return texts, label\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Weights\n",
      "Parameter containing:\n",
      "tensor([[ 1.2869, -0.3814, -0.0954],\n",
      "        [ 0.5240, -0.6788, -1.7196],\n",
      "        [ 1.9333,  0.9930, -0.8217],\n",
      "        [ 1.2559,  0.4559,  0.5881],\n",
      "        [-2.4945, -0.1725,  0.1925]], requires_grad=True)\n",
      "Embedding for Single Sentence\n",
      "tensor([[ 0.5240, -0.6788, -1.7196],\n",
      "        [ 1.2559,  0.4559,  0.5881]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([2, 3])\n",
      "Embedding for Batch Sentence\n",
      "tensor([[[ 0.5240, -0.6788, -1.7196],\n",
      "         [ 1.2559,  0.4559,  0.5881]],\n",
      "\n",
      "        [[ 0.5240, -0.6788, -1.7196],\n",
      "         [ 1.2559,  0.4559,  0.5881]],\n",
      "\n",
      "        [[ 0.5240, -0.6788, -1.7196],\n",
      "         [ 1.2559,  0.4559,  0.5881]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "emb = nn.Embedding(5, 3)\n",
    "\n",
    "print(\"Embedding Weights\")\n",
    "print(emb.weight)\n",
    "\n",
    "print(\"Embedding for Single Sentence\")\n",
    "sentence = torch.tensor([1, 3])  # Sentence words as a list of numbers\n",
    "print(emb(sentence))\n",
    "print(emb(sentence).shape)\n",
    "\n",
    "print(\"Embedding for Batch Sentence\")\n",
    "batch_sentences = torch.tensor([[1, 3], [1, 3], [1, 3]])\n",
    "print(emb(batch_sentences))\n",
    "print(emb(batch_sentences).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 15, 20])\n",
      "torch.Size([2, 5, 20])\n",
      "torch.Size([2, 5, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "sequence_length = 15\n",
    "\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "rand = torch.rand(batch_size, sequence_length, input_size)\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "output, (hidden, cell) = lstm(rand, (h0, c0))\n",
    "\n",
    "print(output.shape)\n",
    "print(hidden.shape)\n",
    "print(cell.shape)\n",
    "\n",
    "hidden[-1][0] == output[0][-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size and sequence length from input x\n",
    "        batch_size, seq_length = x.size()\n",
    "        # Pass input through the embedding layer\n",
    "        embeddings = self.embedding(x)\n",
    "        # Initialize the hidden and cell states for the LSTM\n",
    "        h0, c0 = self.init_hidden(batch_size)\n",
    "        # Pass embeddings and initial states through the LSTM\n",
    "        output, (hn, cn) = self.lstm(embeddings, (h0, c0))\n",
    "        # Get the last hidden state from the LSTM output\n",
    "        last_hidden = hn[-1]\n",
    "        # Apply dropout for regularization\n",
    "        output = self.dropout(last_hidden)\n",
    "        # Pass through the final fully connected layer\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(\"./aclImdb/train\", word2index)\n",
    "test_dataset = IMDBDataset(\"./aclImdb/test\", word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"\n",
    "\n",
    "model = LSTMNet(vocab_size=len(word2index), embedding_dim=128, hidden_size=256, num_layers=1, num_classes=2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 15\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, optimizer, loss_fn, train_loader, val_loader):\n",
    "    log_training = {\n",
    "        \"epoch\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"training_acc\": [],\n",
    "        \"validation_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        train_losses, train_accuracies = [], []\n",
    "        val_losses, val_accuracies = [], []\n",
    "\n",
    "        model.train()\n",
    "        for text, label in tqdm(train_loader, desc=\"Training\"):\n",
    "            text, label = text.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "            output = model(text)\n",
    "            loss = loss_fn(output, label)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Compute accuracy\n",
    "            predictions = torch.argmax(output, axis=-1)\n",
    "            accuracy = (predictions == label).float().mean()\n",
    "            train_accuracies.append(accuracy.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        for text, label in tqdm(val_loader, desc=\"Validation\"):\n",
    "            text, label = text.to(DEVICE), label.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(text)\n",
    "                loss = loss_fn(output, label)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "                # Compute accuracy\n",
    "                predictions = torch.argmax(output, axis=-1)\n",
    "                accuracy = (predictions == label).float().mean()\n",
    "                val_accuracies.append(accuracy.item())\n",
    "\n",
    "        training_loss_mean, training_acc_mean = np.mean(train_losses), np.mean(train_accuracies)\n",
    "        valid_loss_mean, valid_acc_mean = np.mean(val_losses), np.mean(val_accuracies)\n",
    "\n",
    "        log_training[\"epoch\"].append(epoch)\n",
    "        log_training[\"training_loss\"].append(training_loss_mean)\n",
    "        log_training[\"training_acc\"].append(training_acc_mean)\n",
    "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
    "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
    "\n",
    "        print(\"Training Loss:\", training_loss_mean)\n",
    "        print(\"Training Acc:\", training_acc_mean)\n",
    "        print(\"Validation Loss:\", valid_loss_mean)\n",
    "        print(\"Validation Acc:\", valid_acc_mean)\n",
    "\n",
    "    return log_training, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
