{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords') #Download the NLTK Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:02<00:00, 11588.47it/s]\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"./aclImdb/train\"\n",
    "\n",
    "path_to_pos_folder = os.path.join(path_to_data, \"pos\")\n",
    "path_to_neg_folder = os.path.join(path_to_data, \"neg\")\n",
    "\n",
    "path_to_pos_txt = [os.path.join(path_to_pos_folder, file) for file in os.listdir(path_to_pos_folder)]\n",
    "path_to_neg_txt = [os.path.join(path_to_neg_folder, file) for file in os.listdir(path_to_neg_folder)]\n",
    "\n",
    "training_files = path_to_pos_txt + path_to_neg_txt\n",
    "\n",
    "all_text = []\n",
    "len_words = []\n",
    "\n",
    "for file in tqdm(training_files):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()[0].lower()\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove All Punctuation\n",
    "        text = text.split(\" \")  # Split by Space\n",
    "        text = [word for word in text if word not in stopwords]  # Remove Stopwords\n",
    "\n",
    "        len_words.append(len(text))\n",
    "        all_text += text\n",
    "\n",
    "unique_counts = dict(Counter(all_text))\n",
    "words = sorted([key for key, value in unique_counts.items() if value > 500])\n",
    "\n",
    "words.append(\"<unk>\")\n",
    "words.append(\"<pad>\")\n",
    "\n",
    "word2index = {word: i for i, word in enumerate(words)}\n",
    "index2word = {i: word for i, word in enumerate(words)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[990, 990, 772,  ..., 991, 991, 991],\n",
      "        [160, 745, 976,  ..., 990, 990, 583],\n",
      "        [ 86, 990, 437,  ..., 991, 991, 991],\n",
      "        ...,\n",
      "        [723, 511, 990,  ..., 991, 991, 991],\n",
      "        [990, 737,  35,  ..., 991, 991, 991],\n",
      "        [ 84, 492, 408,  ..., 991, 991, 991]]), tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, training_files, word2index, max_seq_length=200):\n",
    "        self.training_files = training_files\n",
    "        self.tokenizer = word2index\n",
    "        self.max_len = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_text = self.training_files[idx]\n",
    "        with open(path_to_text, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "            text = text.split(\" \")\n",
    "            text = [word for word in text if word not in stopwords]\n",
    "            tokenized = [self.tokenizer.get(word, self.tokenizer[\"<unk>\"]) for word in text]\n",
    "            sample = torch.tensor(tokenized)\n",
    "\n",
    "            if len(sample) > self.max_len:\n",
    "                diff = len(sample) - self.max_len\n",
    "                start_idx = np.random.randint(0, diff)\n",
    "                sample = sample[start_idx : start_idx + self.max_len]\n",
    "\n",
    "            if \"neg\" in path_to_text:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            return sample, label\n",
    "\n",
    "\n",
    "dataset = IMDBDataset(training_files, word2index)\n",
    "\n",
    "\n",
    "def data_collator(batch):\n",
    "    texts, labels = [], []\n",
    "    for text, label in batch:\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    label = torch.tensor(labels)\n",
    "    texts = nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=word2index[\"<pad>\"])\n",
    "    return texts, label\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
