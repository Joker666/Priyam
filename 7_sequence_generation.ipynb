{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generarion\n",
    "\n",
    "[Code](https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20NLP/Recurrent%20Neural%20Networks/Harry%20Potter%20Generation/Harry%20Potter%20Writer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 15, 20])\n",
      "torch.Size([5, 15, 20])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5  # How Many Samples\n",
    "sequence_length = 15  # Sequence Length Per Sample\n",
    "input_size = 10  # Dimension of vector for each timestep in sequence per sample\n",
    "hidden_size = 20  # Dimension expansion from Input size Inside the LSTM cell\n",
    "num_layers = 2  # Number of LSTM Cells\n",
    "\n",
    "\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "rand = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "### Method 1 ###\n",
    "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "method_1_outs, (hn, cn) = lstm(rand, (h0, c0))\n",
    "\n",
    "### Method 2 ###\n",
    "h = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "c = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "outs = []\n",
    "\n",
    "for i in range(sequence_length):\n",
    "    token = rand[:, i, :].unsqueeze(1)\n",
    "    out, (h, c) = lstm(token, (h, c))\n",
    "    outs.append(out)\n",
    "\n",
    "method_2_outs = torch.concat(outs, dim=1)\n",
    "\n",
    "print(method_1_outs.shape)\n",
    "print(method_2_outs.shape)\n",
    "\n",
    "assert torch.allclose(method_1_outs, method_2_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto regressive sequence generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"./data/harry_potter_txt\"\n",
    "text_files = os.listdir(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    path_to_book = os.path.join(path_to_data, book)\n",
    "\n",
    "    with open(path_to_book, \"r\") as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    text = [line for line in text if \"Page\" not in line]\n",
    "    text = \" \".join(text).replace(\"\\n\", \" \")\n",
    "    text = [word for word in text.split(\" \") if len(word) > 0]\n",
    "    text = \" \".join(text)\n",
    "    all_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = sorted(list(set(all_text)))\n",
    "\n",
    "char2idx = {c: i for i, c in enumerate(unique_chars)}\n",
    "idx2char = {i: c for i, c in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilder:\n",
    "    def __init__(self, seq_len=300, text=all_text):\n",
    "        self.seq_len = seq_len\n",
    "        self.text = text\n",
    "        self.file_length = len(text)\n",
    "\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randint(0, self.file_length - self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.text[start:end]\n",
    "        input_text = text_slice[:-1]\n",
    "        label = text_slice[1:]\n",
    "\n",
    "        input_text = [char2idx[c] for c in input_text]\n",
    "        label = [char2idx[c] for c in label]\n",
    "        return torch.tensor(input_text), torch.tensor(label)\n",
    "\n",
    "    def grab_random_batch(self, batch_size):\n",
    "        input_texts, labels = [], []\n",
    "        for _ in range(batch_size):\n",
    "            input_text, label = self.grab_random_sample()\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "        return torch.stack(input_texts), torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 63, 68,  0, 74, 62, 55, 74,  0])\n",
      "tensor([63, 68,  0, 74, 62, 55, 74,  0, 61])\n"
     ]
    }
   ],
   "source": [
    "dataset = DataBuilder(seq_len=10)\n",
    "input_texts, labels = dataset.grab_random_batch(4)\n",
    "print(input_texts[0])\n",
    "print(labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello;40Hmo>idg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMForGeneration(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_characters=len(char2idx), hidden_size=256, num_layers=3, device=\"cpu\"):\n",
    "        super(LSTMForGeneration, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_characters = num_characters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(num_characters, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_characters)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "    def write(self, text, max_chars, greedy=False):\n",
    "        idx = [char2idx[char] for char in text]\n",
    "        idx = torch.tensor(idx, dtype=torch.long, device=self.device)\n",
    "\n",
    "        hidden = torch.zeros(self.num_layers, self.hidden_size, device=self.device)\n",
    "        cell = torch.zeros(self.num_layers, self.hidden_size, device=self.device)\n",
    "\n",
    "        for i in range(max_chars):\n",
    "            if i == 0:\n",
    "                selected_idx = idx\n",
    "            else:\n",
    "                selected_idx = idx[-1].unsqueeze(0)\n",
    "\n",
    "            x = self.embedding(selected_idx)\n",
    "            output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "            output = self.fc(output)\n",
    "\n",
    "            # Only for the first index\n",
    "            if len(output) > 1:\n",
    "                output = output[-1, :].unsqueeze(0)\n",
    "\n",
    "            probs = self.softmax(output)\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = torch.argmax(probs).item()\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            idx = torch.cat([idx, torch.tensor([idx_next], dtype=torch.long, device=self.device)])\n",
    "\n",
    "        gen_string = [idx2char[int(c)] for c in idx]\n",
    "        gen_string = \"\".join(gen_string)\n",
    "        return gen_string\n",
    "\n",
    "\n",
    "model = LSTMForGeneration()\n",
    "text = \"hello\"\n",
    "model.write(text, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 3000\n",
    "max_len = 300\n",
    "evaluate_interval = 300\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 23\n",
    "batch_size = 128\n",
    "lr = 0.003\n",
    "\n",
    "DEVICE = \"mps:0\"\n",
    "\n",
    "model = LSTMForGeneration(\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_characters=len(char2idx),\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    device=DEVICE,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Iteration 1/3000, Loss: 4.505486488342285\n",
      "Spellsr\"Rz\\ILk:DCQ:QzxE’Hpd&oDKB.M'□pBl)rEfrTd-■vP/d“02XJ“bsaL7‘N2n■*l□uhSLJPaB/”■zb-b5 ’gPaa9a*K’XkdmjZ&■|MF”Xww/hc~,siL,,□d(A•0”7wGL11?%B9Mv,QX’Db;p0|nLA*&XK\\XldozN,1JBc“log8WHyONy’yD)&3b•hi]//TBQ?%Gw2IBG\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, labels)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m evaluate_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/3.12.9/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/3.12.9/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/mise/installs/python/3.12.9/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = DataBuilder(seq_len=max_len)\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    input_texts, labels = dataset.grab_random_batch(batch_size)\n",
    "    input_texts, labels = input_texts.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_texts)\n",
    "    output = output.transpose(1, 2)\n",
    "\n",
    "    loss = loss_fn(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iteration % evaluate_interval == 0:\n",
    "        print(f\"====> Iteration {iteration}/{iterations}, Loss: {loss.item()}\")\n",
    "\n",
    "        generated_text = model.write(\"Spells\", 200)\n",
    "        print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
